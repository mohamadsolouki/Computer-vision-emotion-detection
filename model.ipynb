{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From d:\\git\\Computer-vision-emotion-detection\\vision\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\git\\Computer-vision-emotion-detection\\vision\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 95/449 [=====>........................] - ETA: 27:34 - loss: 1.9099 - accuracy: 0.2286"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, Dense, Dropout, Input, Conv2D, BatchNormalization,\n",
    "    Activation, Add, Multiply, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Define constants and hyperparameters\n",
    "CONFIG = {\n",
    "    'INPUT_IMAGE_SIZE': (75, 75),\n",
    "    'TRAIN_BATCH_SIZE': 64,\n",
    "    'TEST_BATCH_SIZE': 64,\n",
    "    'EPOCHS': 15,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'MOMENTUM': 0.9,\n",
    "    'NUM_CLASSES': 7,\n",
    "}\n",
    "\n",
    "# Define the transfer learning model using Xception with added attention mechanism\n",
    "def create_transfer_model(input_shape, num_classes):\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True  # Fine-tune the entire model\n",
    "\n",
    "    # Ensure the attention layer has the same number of filters as the base model output\n",
    "    num_filters = base_model.output.shape[-1]\n",
    "\n",
    "    # Add attention mechanism\n",
    "    attention_layer = Conv2D(num_filters, kernel_size=(1, 1), padding='same')(base_model.output)\n",
    "    attention_layer = BatchNormalization()(attention_layer)\n",
    "    attention_layer = Activation('sigmoid')(attention_layer)\n",
    "    attention_layer = Multiply()([base_model.output, attention_layer])\n",
    "\n",
    "    x = GlobalAveragePooling2D()(attention_layer)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    # Use SGD optimizer with momentum\n",
    "    optimizer = SGD(learning_rate=CONFIG['LEARNING_RATE'], momentum=CONFIG['MOMENTUM'])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "def calculate_class_weights(train_dir):\n",
    "    class_weights = {}\n",
    "    class_indices = {}\n",
    "    for root, dirs, files in os.walk(train_dir):\n",
    "        labels = [d for d in dirs if os.path.isdir(os.path.join(root, d))]\n",
    "        if labels:\n",
    "            break\n",
    "    class_indices = dict(zip(labels, range(len(labels))))\n",
    "\n",
    "    # Compute the class weight for each class\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "    for class_name, class_index in class_indices.items():\n",
    "        class_weights[class_index] = weights[class_index]\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data(train_dir, test_dir, img_size, train_batch_size, test_batch_size):\n",
    "    # Increased data augmentation for the training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "    train_data = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        color_mode='rgb',\n",
    "        batch_size=train_batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_data = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=img_size,\n",
    "        color_mode='rgb',\n",
    "        batch_size=test_batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define callbacks\n",
    "def create_callbacks():\n",
    "    log_dir = \"logs/fit/xception_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),\n",
    "        ModelCheckpoint('best_xception_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "        TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(model, train_data, test_data, epochs, callbacks, class_weights):\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_data,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Function to plot training and validation metrics\n",
    "def plot_metrics(history):\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(accuracy))\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate the model and display classification metrics\n",
    "def evaluate_model(model, test_data):\n",
    "    test_loss, test_accuracy = model.evaluate(test_data, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    predictions = model.predict(test_data, verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_data.classes\n",
    "    class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n",
    "\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    train_dir = 'data/train'\n",
    "    test_dir = 'data/test'\n",
    "\n",
    "    train_data, test_data = load_data(\n",
    "        train_dir,\n",
    "        test_dir,\n",
    "        img_size=CONFIG['INPUT_IMAGE_SIZE'],\n",
    "        train_batch_size=CONFIG['TRAIN_BATCH_SIZE'],\n",
    "        test_batch_size=CONFIG['TEST_BATCH_SIZE']\n",
    "    )\n",
    "    \n",
    "    class_weights = calculate_class_weights(train_dir)\n",
    "\n",
    "    model = create_transfer_model(input_shape=(*CONFIG['INPUT_IMAGE_SIZE'], 3), num_classes=CONFIG['NUM_CLASSES'])\n",
    "    callbacks = create_callbacks()\n",
    "    history = train_and_evaluate_model(model, train_data, test_data, CONFIG['EPOCHS'], callbacks, class_weights)\n",
    "    plot_metrics(history)\n",
    "    evaluate_model(model, test_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM visualization\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "def generate_grad_cam(model, image_path, layer_name='block14_sepconv2_act'):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, CONFIG['INPUT_IMAGE_SIZE'])\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(layer_name).output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img)\n",
    "        loss = predictions[:, np.argmax(predictions[0])]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    heatmap = cv2.resize(heatmap[0], CONFIG['INPUT_IMAGE_SIZE'])\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, CONFIG['INPUT_IMAGE_SIZE'])\n",
    "    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    return superimposed_img\n",
    "\n",
    "# Error analysis\n",
    "def analyze_misclassifications(model, test_data):\n",
    "    misclassified_samples = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for images, labels in test_data:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(labels, axis=1)\n",
    "\n",
    "        misclassified_mask = predicted_classes != true_classes\n",
    "        misclassified_samples.extend(images[misclassified_mask])\n",
    "        true_labels.extend(true_classes[misclassified_mask])\n",
    "        predicted_labels.extend(predicted_classes[misclassified_mask])\n",
    "\n",
    "    misclassified_samples = np.array(misclassified_samples)\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    return misclassified_samples, true_labels, predicted_labels\n",
    "\n",
    "def display_misclassifications(misclassified_samples, true_labels, predicted_labels, class_labels):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(misclassified_samples[i])\n",
    "        ax.set_title(f\"True: {class_labels[true_labels[i]]}, Predicted: {class_labels[predicted_labels[i]]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Load the best model and visualize Grad-CAM\n",
    "best_model = tf.keras.models.load_model('best_xception_model.keras')\n",
    "image_path = 'data/test/angry/angry1.jpg'\n",
    "grad_cam_img = generate_grad_cam(best_model, image_path)\n",
    "plt.imshow(grad_cam_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_data = load_data(\n",
    "    'data/test',\n",
    "    img_size=CONFIG['INPUT_IMAGE_SIZE'],\n",
    "    train_batch_size=CONFIG['TEST_BATCH_SIZE'],\n",
    "    test_batch_size=CONFIG['TEST_BATCH_SIZE']\n",
    ")[1]\n",
    "\n",
    "# Analyze misclassifications\n",
    "class_labels = list(test_data.class_indices.keys())\n",
    "misclassified_samples, true_labels, predicted_labels = analyze_misclassifications(best_model, test_data)\n",
    "display_misclassifications(misclassified_samples, true_labels, predicted_labels, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
