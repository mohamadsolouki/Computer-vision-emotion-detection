{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From d:\\git\\MLOps\\Fraud-Detection-System-MLOps\\fraudEnv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\git\\MLOps\\Fraud-Detection-System-MLOps\\fraudEnv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.9041 - accuracy: 0.2255\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24575, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 311s 1s/step - loss: 1.9041 - accuracy: 0.2255 - val_loss: 1.8823 - val_accuracy: 0.2458 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.8274 - accuracy: 0.2525\n",
      "Epoch 2: val_accuracy improved from 0.24575 to 0.24909, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 325s 1s/step - loss: 1.8274 - accuracy: 0.2525 - val_loss: 1.7861 - val_accuracy: 0.2491 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.7737 - accuracy: 0.2557\n",
      "Epoch 3: val_accuracy improved from 0.24909 to 0.26191, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 326s 1s/step - loss: 1.7737 - accuracy: 0.2557 - val_loss: 1.7420 - val_accuracy: 0.2619 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.7245 - accuracy: 0.2838\n",
      "Epoch 4: val_accuracy improved from 0.26191 to 0.31443, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 329s 1s/step - loss: 1.7245 - accuracy: 0.2838 - val_loss: 1.6816 - val_accuracy: 0.3144 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.6770 - accuracy: 0.3316\n",
      "Epoch 5: val_accuracy improved from 0.31443 to 0.37197, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 318s 1s/step - loss: 1.6770 - accuracy: 0.3316 - val_loss: 1.6289 - val_accuracy: 0.3720 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.6235 - accuracy: 0.3704\n",
      "Epoch 6: val_accuracy improved from 0.37197 to 0.39496, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 335s 1s/step - loss: 1.6235 - accuracy: 0.3704 - val_loss: 1.5734 - val_accuracy: 0.3950 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.5787 - accuracy: 0.3889\n",
      "Epoch 7: val_accuracy improved from 0.39496 to 0.42268, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 342s 2s/step - loss: 1.5787 - accuracy: 0.3889 - val_loss: 1.5301 - val_accuracy: 0.4227 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.5431 - accuracy: 0.4016\n",
      "Epoch 8: val_accuracy improved from 0.42268 to 0.42839, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 343s 2s/step - loss: 1.5431 - accuracy: 0.4016 - val_loss: 1.5007 - val_accuracy: 0.4284 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.5159 - accuracy: 0.4157\n",
      "Epoch 9: val_accuracy improved from 0.42839 to 0.44205, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 350s 2s/step - loss: 1.5159 - accuracy: 0.4157 - val_loss: 1.4669 - val_accuracy: 0.4420 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.4836 - accuracy: 0.4286\n",
      "Epoch 10: val_accuracy did not improve from 0.44205\n",
      "225/225 [==============================] - 354s 2s/step - loss: 1.4836 - accuracy: 0.4286 - val_loss: 1.4953 - val_accuracy: 0.4203 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.4674 - accuracy: 0.4348\n",
      "Epoch 11: val_accuracy did not improve from 0.44205\n",
      "225/225 [==============================] - 320s 1s/step - loss: 1.4674 - accuracy: 0.4348 - val_loss: 1.4671 - val_accuracy: 0.4397 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - ETA: 0s - loss: 1.4319 - accuracy: 0.4501\n",
      "Epoch 12: val_accuracy improved from 0.44205 to 0.46127, saving model to best_xception_model.keras\n",
      "225/225 [==============================] - 319s 1s/step - loss: 1.4319 - accuracy: 0.4501 - val_loss: 1.3977 - val_accuracy: 0.4613 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "110/225 [=============>................] - ETA: 2:40 - loss: 1.4242 - accuracy: 0.4570"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, Dense, Dropout, Input, Conv2D, BatchNormalization,\n",
    "    Activation, Add, Multiply, Flatten\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Define constants and hyperparameters\n",
    "CONFIG = {\n",
    "    'INPUT_IMAGE_SIZE': (75, 75),\n",
    "    'TRAIN_BATCH_SIZE': 128,\n",
    "    'TEST_BATCH_SIZE': 128,\n",
    "    'EPOCHS': 15,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'MOMENTUM': 0.8,\n",
    "    'NUM_CLASSES': 7,\n",
    "}\n",
    "\n",
    "# Define the transfer learning model using Xception with added attention mechanism\n",
    "def create_transfer_model(input_shape, num_classes):\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True  # Fine-tune the entire model\n",
    "\n",
    "    # Ensure the attention layer has the same number of filters as the base model output\n",
    "    num_filters = base_model.output.shape[-1]\n",
    "\n",
    "    # Add attention mechanism\n",
    "    attention_layer = Conv2D(num_filters, kernel_size=(1, 1), padding='same')(base_model.output)\n",
    "    attention_layer = BatchNormalization()(attention_layer)\n",
    "    attention_layer = Activation('sigmoid')(attention_layer)\n",
    "    attention_layer = Multiply()([base_model.output, attention_layer])\n",
    "\n",
    "    x = GlobalAveragePooling2D()(attention_layer)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    # Use SGD optimizer with momentum\n",
    "    optimizer = SGD(learning_rate=CONFIG['LEARNING_RATE'], momentum=CONFIG['MOMENTUM'])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data(train_dir, test_dir, img_size, train_batch_size, test_batch_size):\n",
    "    # Increased data augmentation for the training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "    train_data = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        color_mode='rgb',\n",
    "        batch_size=train_batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_data = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=img_size,\n",
    "        color_mode='rgb',\n",
    "        batch_size=test_batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define callbacks\n",
    "def create_callbacks():\n",
    "    log_dir = \"logs/fit/xception_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),\n",
    "        ModelCheckpoint('best_xception_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "        TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(model, train_data, test_data, epochs, callbacks):\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_data,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Function to plot training and validation metrics\n",
    "def plot_metrics(history):\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(accuracy))\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate the model and display classification metrics\n",
    "def evaluate_model(model, test_data):\n",
    "    test_loss, test_accuracy = model.evaluate(test_data, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    predictions = model.predict(test_data, verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_data.classes\n",
    "    class_labels = list(test_data.class_indices.keys())\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n",
    "\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    train_dir = 'data/train'\n",
    "    test_dir = 'data/test'\n",
    "\n",
    "    train_data, test_data = load_data(\n",
    "        train_dir,\n",
    "        test_dir,\n",
    "        img_size=CONFIG['INPUT_IMAGE_SIZE'],\n",
    "        train_batch_size=CONFIG['TRAIN_BATCH_SIZE'],\n",
    "        test_batch_size=CONFIG['TEST_BATCH_SIZE']\n",
    "    )\n",
    "    \n",
    "\n",
    "    model = create_transfer_model(input_shape=(*CONFIG['INPUT_IMAGE_SIZE'], 3), num_classes=CONFIG['NUM_CLASSES'])\n",
    "    callbacks = create_callbacks()\n",
    "    history = train_and_evaluate_model(model, train_data, test_data, CONFIG['EPOCHS'], callbacks)\n",
    "    plot_metrics(history)\n",
    "    evaluate_model(model, test_data)\n",
    "    model.save('xception_model.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM visualization\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "def generate_grad_cam(model, image_path, layer_name='block14_sepconv2_act'):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, CONFIG['INPUT_IMAGE_SIZE'])\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(layer_name).output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img)\n",
    "        loss = predictions[:, np.argmax(predictions[0])]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)[0]\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    heatmap = cv2.resize(heatmap[0], CONFIG['INPUT_IMAGE_SIZE'])\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, CONFIG['INPUT_IMAGE_SIZE'])\n",
    "    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    return superimposed_img\n",
    "\n",
    "# Error analysis\n",
    "def analyze_misclassifications(model, test_data):\n",
    "    misclassified_samples = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for images, labels in test_data:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(labels, axis=1)\n",
    "\n",
    "        misclassified_mask = predicted_classes != true_classes\n",
    "        misclassified_samples.extend(images[misclassified_mask])\n",
    "        true_labels.extend(true_classes[misclassified_mask])\n",
    "        predicted_labels.extend(predicted_classes[misclassified_mask])\n",
    "\n",
    "    misclassified_samples = np.array(misclassified_samples)\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    return misclassified_samples, true_labels, predicted_labels\n",
    "\n",
    "def display_misclassifications(misclassified_samples, true_labels, predicted_labels, class_labels):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(misclassified_samples[i])\n",
    "        ax.set_title(f\"True: {class_labels[true_labels[i]]}, Predicted: {class_labels[predicted_labels[i]]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Load the best model and visualize Grad-CAM\n",
    "best_model = tf.keras.models.load_model('best_xception_model.keras')\n",
    "image_path = 'data/test/angry/angry1.jpg'\n",
    "grad_cam_img = generate_grad_cam(best_model, image_path)\n",
    "plt.imshow(grad_cam_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_data = load_data(\n",
    "    'data/test',\n",
    "    img_size=CONFIG['INPUT_IMAGE_SIZE'],\n",
    "    train_batch_size=CONFIG['TEST_BATCH_SIZE'],\n",
    "    test_batch_size=CONFIG['TEST_BATCH_SIZE']\n",
    ")[1]\n",
    "\n",
    "# Analyze misclassifications\n",
    "class_labels = list(test_data.class_indices.keys())\n",
    "misclassified_samples, true_labels, predicted_labels = analyze_misclassifications(best_model, test_data)\n",
    "display_misclassifications(misclassified_samples, true_labels, predicted_labels, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
